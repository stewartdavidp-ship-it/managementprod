# CLAUDE.md — Command Center

## What This App Is
AI-powered ideation and development platform with ODRC framework. Three-tab UI (Projects, Jobs, Sessions) serving as a read-only visualization layer over Firebase data managed by Chat and Code via MCP.

## Current Build Objective
**CC Retrospective Journal**

A shared MCP skill (cc-retro-journal) that Chat and Code both read on startup and write to at job completion. Captures delegation insights, process improvements, anti-patterns, and tool usage observations. Two-section format: Distilled Patterns (consolidated wisdom) and Recent Entries (raw retro log). Periodic consolidation keeps the skill within size limits while accumulating organizational learning across conversations.

## RULEs — Do not violate these.

- CC is the journal of record, not the gatekeeper. _(from: Session tab)_
- Code must output a completion file after every task. This is enforced via CLAUDE.md RULE with pre-completion checklist framing. _(from: Completion Files & Code Contract)_
- One active Code job per repo at a time. CC enforces sequential execution to prevent merge conflicts. _(from: Completion Files & Code Contract)_
- All shared Firebase-backed data lives as top-level state in the App component with global prefix. No view component owns shared data. _(from: CC Architecture Rules)_
- Firebase listeners are set up once in the App component's auth useEffect. Views never create their own listeners for shared data. _(from: CC Architecture Rules)_
- Views own local UI state only — filters, modal open/close, form inputs, selected items. Never data another view needs. _(from: CC Architecture Rules)_
- Write to Firebase via service methods, let listener update state. No optimistic UI updates. This prevents local state and Firebase from diverging. _(from: CC Architecture Rules)_
- Data flows down via props, events flow up via callbacks. No component reaches up or sideways. _(from: CC Architecture Rules)_
- Service objects are global singletons callable from any component. They are the write path to Firebase. _(from: CC Architecture Rules)_
- One listener per collection per user. Never two listeners on the same Firebase path. _(from: CC Architecture Rules)_
- Listener callbacks only call the state setter. No side effects, no cascading writes. _(from: CC Architecture Rules)_
- All listener useEffect blocks must return a cleanup function. No orphaned listeners. _(from: CC Architecture Rules)_
- Serialize by design, not by code. If two operations could modify the same Firebase path, the UI must prevent concurrent access. _(from: CC Architecture Rules)_
- Use Firebase multi-path updates when multiple writes must be atomic. _(from: CC Architecture Rules)_
- Stress test recommended at two lifecycle points: early (session 2-3) to test "is this worth pursuing" and pre-spec to test "is this ready to build" — but never required, surfaced as suggestions by CC _(from: Defined Skills for Cloud Chat)_
- Each persona lens should produce a meaningfully different set of ODRC outputs from the same idea — if two personas surface the same issues, the lenses aren't differentiated enough _(from: Defined Skills for Cloud Chat)_
- Every session must produce a minimum session package: ODRC updates, session brief, and session handoff. No exceptions. CC flags missing components during ingestion. _(from: Defined Skills for Cloud Chat)_
- Skills are the single source of truth for session behavior. Briefs never contain behavioral instructions — only context. If a behavioral pattern needs to change, it's a skill version update, not a brief template change. _(from: Defined Skills for Cloud Chat)_
- Any protocol that needs to survive context loss must use the filesystem as its durable layer. Chat's memory is unreliable — the session manifest, transcripts, and skills all live on disk and can be re-read at any time. _(from: Defined Skills for Cloud Chat)_
- At session close, Chat must re-read the session close skill before producing output, regardless of session length or perceived context retention. The skill includes this self-reinforcing instruction. _(from: Defined Skills for Cloud Chat)_
- CC is invisible infrastructure. The developer doesn't want to be in CC — they want to be in Claude. CC adds value by making AI sessions dramatically better through context assembly and continuity tracking. The less time spent in CC, the better the product is performing. Design every interaction to minimize clicks and get the developer out fast. _(from: Session tab)_
- CC is a prep engine, not an integration point. CC cannot programmatically inject context into Claude Chat or Claude Code. It maximizes handoff quality — generating briefs, building prompts, downloading files, copying to clipboard — while the developer completes the last-mile manual steps. Design within this constraint. _(from: Session tab)_
- Don't guess what developers want on the landing page — ship the minimum and let usage tell you what's missing. Aggregate stats like total OPENs across all Ideas were cut because they raise questions without providing actionable answers. _(from: Session tab)_
- Anything consistent across a majority of sessions becomes a skill. Anything dynamic and based on historical context stays in the brief. This is the governing principle for what goes where. _(from: Defined Skills for Cloud Chat)_
- Each session should target a specific goal through a focused lens — broad unfocused sessions miss gaps. Focusing on specific areas (technical, economics, competitive, voice of customer) allows deeper examination and surfaces real Constraints and Rules that broad sessions miss. _(from: Defined Skills for Cloud Chat)_
- Machine-parseable data lives in JSON, not extracted from markdown prose — markdown is for human/AI-readable narrative, JSON is for structured data the processor consumes _(from: Session tab)_
- Every file in the zip must be referenced in the session.json artifacts array — the JSON is the manifest; if it's not listed, it doesn't get processed _(from: Session tab)_
- Start with a minimal viable session schema and iterate based on actual usage patterns — don't try to anticipate every field _(from: Session tab)_
- Shorter high-quality sessions are preferred over longer degrading ones. Make transitions cheap so developers start fresh rather than squeezing exhausted context. _(from: Session tab)_
- Write session output while context is fresh, not at session close. Link briefs capture during the link; summation synthesizes from link briefs — output never depends on degraded context. _(from: Session tab)_
- Plan chains as N+1: N working links plus 1 summation link. Budget for summation as part of the chain, not an afterthought. _(from: Session tab)_
- Transcripts are primary source in summation; link briefs are fallback. Use the highest-fidelity source available. _(from: Session tab)_
- Tangent OPENs are captured with idea affinity at moment of capture, not deferred to triage. Ask 'does this relate to an existing idea?' immediately. _(from: Session tab)_
- Don't stack measurement before validating adoption of the simplest version. One rating before two; ratio before algorithms. _(from: Session tab)_
- Don't over-invest in phase calculation sophistication — the value is a quick gut-check, not precision. _(from: Session tab)_
- Chain length guided by 3-5 recommendation; beyond 5 is a risk signal for context fatigue and summation quality degradation. _(from: Session tab)_
- Don't delete cc-post-session-package until cc-summation-link is validated through at least one complete chain. _(from: Session tab)_
- Keep the session-to-idea interface simple and rigid. Complexity belongs in CC's backend management layer, not in the session protocol. This separation means session skills never need to account for multi-idea scenarios. _(from: Session tab)_
- Every new idea defaults to inception state. Developer can override at creation. First session for inception-state ideas must validate the idea definition before deep exploration. _(from: Session tab)_
- State validation is a brief calibration exercise, not an interrogation. Three to four questions, first five minutes. If answers are confident, move on. If vague, suggest stepping back. Never block forward progress. _(from: Session tab)_
- If a behavior can't be captured in 2-3 sentences of principle, it's too prescriptive for a skill. Simplify to intent and trust Chat's reasoning. Iterate based on observed session quality. _(from: Session tab)_
- Elapsed time and concept block count are both captured per link in the link brief metadata header alongside existing fields (purpose declared, purpose achieved). _(from: Session tab)_
- Concept block thresholds (5-6, 8-9, 12+) are starting values; CC should refine based on actual usage data. _(from: Session tab)_
- When Code acknowledges a task, the ack must include: (1) a numbered step plan of what it will do, (2) a WAIT signal — either "WAIT: poll" for quick lookups under 60 seconds, or "WAIT: working" for multi-step implementation tasks. For "WAIT: working" tasks, Code must send TYPE: info progress messages at each meaningful checkpoint (e.g., code written, tests running, tests passed, deploying, deployed). Silence longer than 2 minutes without a progress update violates this rule. _(from: Defined Skills for Cloud Chat)_
- Chat's polling behavior is governed by Code's WAIT signal. On "WAIT: poll" — Chat stays polling, expects a reply within 60 seconds. On "WAIT: working" — Chat tells Dave what Code is doing and stops polling. Chat informs Dave to re-engage when ready, at which point Chat checks for messages. This prevents wasting context window on empty poll loops during multi-step implementation tasks. _(from: Defined Skills for Cloud Chat)_
- If Code encounters repeated failures (2+ retry cycles on the same step), unexpected blockers, or a task is taking significantly longer than the step plan implied, Code must send a TYPE: info message to Chat explaining what's failing, what it has tried, and whether it needs Chat input or Dave escalation. Code should not silently retry indefinitely — after 3 failed attempts at the same approach, Code must escalate with a TYPE: question or TYPE: escalate message rather than continuing to burn cycles. _(from: Defined Skills for Cloud Chat)_
- The session object in Firebase is the single source of truth for Chat state. Not held in Chat's context window — persisted in Firebase so any Chat instance can read and resume. On every CC interaction, Chat checks for an active session first. If one exists, read it and resume. If none, start in base state. _(from: Session State Machine — Persistent Modes, Commands, and Context Tracking)_
- "what state am I in" and "what commands are available" must work at any time regardless of mode. Chat reads the session object from Firebase and returns: current mode, active idea/app, concepts created count, estimated context usage, any pending Code messages, and the list of valid commands for the current mode with brief explanations. _(from: Session State Machine — Persistent Modes, Commands, and Context Tracking)_
- CC's workflow philosophy is to assist natural processes, not force formal ones. The system should add value with low friction — capturing decisions, discoveries, and constraints as they naturally emerge from conversation or coding, not requiring users to enter a formal mode to produce tracked output. The ODRC framework is a capture mechanism, not a ceremony. _(from: Session State Machine — Persistent Modes, Commands, and Context Tracking)_
- deliver-to-github (and any future delivery action) should delete the document from Firebase after successful delivery. The document served its purpose as a hold-and-forward queue entry — once it's committed to GitHub, keeping it in Firebase is waste. Failed deliveries stay in Firebase for retry/debugging. _(from: Document Queue Lifecycle — TTL, Auto-Cleanup, and Message Hygiene)_
- Messages (type=message) with status 'delivered' (i.e., acked) should be deleted from Firebase immediately after ack. Inter-agent messages are transient coordination — once both sides have read them, there's no reason to persist. If audit trail is needed later, that's a separate logging concern, not the message queue's job. _(from: Document Queue Lifecycle — TTL, Auto-Cleanup, and Message Hygiene)_
- Everything Chat wants Code to do is a job. Session briefs, skill updates, builds, cleanup tasks, bug fixes — all jobs. Anything that crosses the Chat→Code boundary gets a job created. This is the universal rule: if it's work for Code, it's a draft job. _(from: Job as Universal Work Order — Chat→Code Handoff Redesign)_
- Job is compaction-resilient by design. Because the job is persisted in Firebase with all context embedded (instructions, attachments, concept snapshots), Code can always re-read it on resume after compaction. No context recovery dance needed — the job IS the context. _(from: Job as Universal Work Order — Chat→Code Handoff Redesign)_
- Chat can update attachments and instructions on draft jobs before Code claims them. Once claimed (status=active), the job is locked — no more modifications to the work order. This allows Chat to iterate on a draft without creating new jobs. _(from: Job as Universal Work Order — Chat→Code Handoff Redesign)_
- Multiple draft jobs per app are allowed, but only one active build job per app is enforced at claim time. This preserves the existing "one active Code job per repo" rule while allowing Chat to queue up future work as drafts. _(from: Job as Universal Work Order — Chat→Code Handoff Redesign)_
- Every MCP tool response must return the minimum data needed for the task at hand. List calls return summary fields by default (id, name, status, key dates) — not full objects with event arrays, session logs, or attachments. Full records are retrieved via get with a specific ID. Default limit of 20 items on all list endpoints. This is a cost and usability constraint: every token in a tool response is a token the user pays for and a token consumed from the context window. _(from: Context Window Longevity — Lean Startup, Skill Tiering, and Hygiene Checks)_
- Every E2E test must include teardown that deletes all test data it creates in Firebase. This is not optional cleanup — it is a required part of test design. Tests that create sessions, ideas, jobs, concepts, or documents must delete them on completion. The test suite should leave zero artifacts in Firebase after a full run. _(from: CC Maintenance Backlog)_
- Archived ideas are excluded from all ranking surfaces (Chat startup and CC Home). They never appear in the tiered priority list. They remain searchable/listable if the user explicitly requests them (e.g., "show archived ideas") but are filtered out of all default views. _(from: Ideation Platform)_
- cc-session-protocol must include a directive: when transitioning from ideation to job creation (any job type), Chat must read cc-job-creation-protocol before creating the job. Same self-reinforcing pattern as the session close skill — the trigger is in the session protocol, the procedure is in the job creation skill. This ensures Chat never creates a job without following the standardized process, regardless of context window state. _(from: Ideation Platform)_
- cc-skill-router is read once at startup by both Chat and Code. Chat reads it when cc-session-protocol fires on cold start. Code reads it when cc-build-protocol fires on fresh start. The router is NOT re-read per command — it's loaded into context once and referenced from memory for the duration of the session. Only re-read on compaction recovery (since context is lost). _(from: Defined Skills for Cloud Chat)_
- The bootstrap chain is: project system prompt (Chat) or CLAUDE.md (Code) → master skill (cc-session-protocol / cc-build-protocol) → cc-skill-router. The project prompt and CLAUDE.md are the only truly "always present" artifacts — they're the cold start triggers that survive any context loss. The master skills are hardcoded in those triggers. The router is loaded by the master skills, never self-bootstrapped. On compaction recovery, the master skill detects the situation (active session in Firebase, orphaned job) and triggers the appropriate resume skill + re-reads the router. The resume skills (cc-session-resume, cc-build-resume) must be referenced directly in the master skills, not only in the router, because the router may not be in context after compaction. _(from: Defined Skills for Cloud Chat)_
- CC Logo: Hexagonal cluster ("Structured Cells") in amber (#e8a838). Center hex at full intensity, 3 satellite hexes (top-right, bottom-center, left) with stepped-down fill-opacity (.45/.3/.2) and stroke-opacity (.65/.4/.25). Center hex elevated with subtle drop shadow (dx:0.8, dy:0.8, stdDeviation:1.0, opacity:.35) creating 3D layered effect. Gradient shadow in overlap zone between center and top-right. SVG scales from 80px down to 16px with proportionally thicker strokes at small sizes. _(from: CC Brand Identity & Logo)_
- The journal skill must stay under 80 lines total. On startup, if either agent reads the skill and sees Recent Entries exceeding 15 items, it flags to the user that consolidation is due. Chat performs the consolidation — reading entries, distilling patterns, pruning — and ships a skill-update job. If Distilled Patterns itself grows past 30 lines, Chat must also prune: merge overlapping patterns, remove any that have become obvious or are no longer relevant. The skill never grows — it refines. _(from: CC Retrospective Journal)_
- On job completion, Code must call mark_built on every concept ID listed in the job's "Concepts Addressed" section and populate the job's conceptsAddressed array. This is a pre-completion checklist item — the job is not done until concepts are closed out. Chat includes concept IDs in job instructions specifically for this purpose; skipping this step creates orphaned "active" decisions that were actually implemented, degrading idea lifecycle accuracy. _(from: CC Maintenance Backlog)_

## CONSTRAINTs — External realities. Work within these.

- No role-playing as named real people — personas are archetypes only. Prevents gaming caricatures and keeps the exercise focused on transferable skills _(from: Defined Skills for Cloud Chat)_
- Maximum five personas to start — depth and true differentiation over breadth _(from: Defined Skills for Cloud Chat)_
- The Anthropic Skills API is currently in beta (skills-2025-10-02). CC's integration depends on this API remaining available and stable. Breaking changes would require CC adaptation. _(from: Defined Skills for Cloud Chat)_
- Skills operate in a sandboxed execution environment with no data persistence between sessions. Skill definitions persist at the workspace level, but runtime state (files created during a session) does not carry over. _(from: Defined Skills for Cloud Chat)_
- Maximum 8 skills can be attached per API request. The base package plus session-type-specific skills must fit within this limit, constraining how granularly skills can be decomposed. _(from: Defined Skills for Cloud Chat)_
- No programmatic integration with Claude Chat or Claude Code exists today. Launch sequences must use manual handoff patterns: file download, clipboard copy, and developer navigation. Design should not depend on future integration capabilities but should not block them either. _(from: Session tab)_
- Claude.ai custom skill limit is approximately 20 skills. Current plan is 13 skills (4 core + 5 lenses + 4 modes). Leaves headroom for future additions but requires awareness when adding new skills. _(from: Defined Skills for Cloud Chat)_
- SKILL.md limited to 500 lines per skill in claude.ai. Description limited to 200 characters. Skill name limited to 64 characters, lowercase letters/numbers/hyphens only. Reference files can be used for overflow content. _(from: Defined Skills for Cloud Chat)_
- Skills uploaded to claude.ai Settings are individual user only — no programmatic deployment from CC to claude.ai. CC can author and package skills but the developer must manually upload to Settings → Capabilities. _(from: Defined Skills for Cloud Chat)_
- The inbound data processor is currently being rewritten due to the homepage redesign — packaging decisions must be compatible with the new processor but can't design against its specifics yet _(from: Session tab)_
- The ODRC output format in cc-odrc-framework is the existing parser contract — until the new JSON import is built, sessions must continue producing the current markdown format _(from: Session tab)_
- Claude Chat has finite context windows. Packaging workflows can't be heavy when context is strained — this is why summation is a dedicated link, not appended to the last working link. _(from: Session tab)_
- Context windows are finite. Packaging can't be heavy when context is strained — the chain model distributes capture across links to avoid end-of-session quality collapse. _(from: Session tab)_
- File System Access API (showDirectoryPicker) is not supported in Safari or Firefox — only Chromium browsers (Chrome, Edge). Directory watching for session package detection requires Chrome. All-browser fallback uses state-based staleness warnings. _(from: Session tab)_
- Claude.ai clipboard limitations prevent automating the two-step prompt paste — any solution must work within manual copy/paste workflows. _(from: Session tab)_
- All apps currently run under a single Firebase project called "word-boxing." This includes Game Shelf games, Command Center, the MCP server (Cloud Run), Stripe payment functions, RTDB data for both games and ideation, and standalone experiments. The project name causes confusion since it implies a single-game scope. _(from: Platform Architecture — Infrastructure, Firebase Projects, and Security Boundaries)_
- Economics of three projects vs one is roughly neutral. Each Firebase project gets its own free tier allocations (RTDB, hosting, auth, Cloud Functions). At current usage levels, three projects may actually cost less than one since each gets independent free quotas. The Blaze pay-as-you-go model means no fixed cost increase from having more projects. _(from: Platform Architecture — Infrastructure, Firebase Projects, and Security Boundaries)_
- Current hosting topology: Game Shelf apps use a mix of Firebase Hosting and GitHub Pages. Command Center is GitHub Pages. MCP server is Cloud Run. Standalone apps (quotle-info, labelkeeper, etc.) are GitHub Pages. Cloud Functions run under the word-boxing project for game hints, Stripe payments, and the gift/Goody system. RTDB holds both game data and CC ideation data under a single database. _(from: Platform Architecture — Infrastructure, Firebase Projects, and Security Boundaries)_
- Backward compatibility preserved — existing Code-created active jobs (no draft phase) continue to work. The draft flow is purely additive. All new fields (instructions, attachments, conceptSnapshot, createdBy, jobType) are optional on the job start action. _(from: Job as Universal Work Order — Chat→Code Handoff Redesign)_
- Constellation metaphor resonates conceptually (scattered ideas forming a connected pattern) but fails at small icon sizes — connected dots become noise at 32px. The logo needs to capture the essence of 'points becoming pattern' in a form that reads at favicon scale. _(from: CC Brand Identity & Logo)_
- Three critical secret values are stored in localStorage by the Infrastructure satellite and must be preserved when satellites are removed: (1) cc_firebase_sa — Firebase Service Account JSON with private key, enables admin API access for rules, functions, auth config. (2) cc_domain_config — Porkbun API key + secret for DNS management. (3) cc_godaddy_config — GoDaddy API key + secret for DNS management. These credentials currently have no Firebase-synced backup. They must be migrated to the main CC Setup area's credential management before the satellite is removed. _(from: Cc Navigation Redesign)_

## DECISIONs — Current direction for this phase.

- The retrospective journal is an MCP skill (cc-retro-journal) read by both Chat and Code on startup via the skill router. It is the single shared learning surface — both agents read the same document and both contribute entries. Storage as a skill means it's always in context without an extra fetch, and the skill size limit (~500 lines) naturally forces consolidation.
- Chat updates the journal skill by creating a skill-update job for Code. Code updates the journal skill directly since it has repo access. For the initial version, Code simply appends to the skill file in the MCP server's skill storage. The consolidation step (distilling patterns from ~20 raw entries) is a Chat task — Chat reads the recent entries, identifies recurring themes, writes distilled patterns, and creates a skill-update job to apply them.
- Journal entries are written at the end of every job ONLY when something genuinely surprising or non-obvious was learned. The bar: "Would a future Chat or Code instance make a worse decision without knowing this?" If the answer is no, don't write an entry. Routine jobs that confirm existing patterns get no entry. This is not a log — it's a collection of discoveries that change behavior.
- The skill has two sections: (1) Distilled Patterns — consolidated actionable rules (max ~30 lines), and (2) Recent Entries — raw discoveries (max ~40 lines, roughly 10-15 entries). Total skill target: under 80 lines. When Recent Entries exceeds 15 entries, a consolidation exercise is triggered. Chat owns consolidation — it reads recent entries, identifies recurring themes, distills them into 1-2 new pattern lines, and removes the consumed entries via a skill-update job. Consolidation is a Chat task because it requires judgment about what patterns matter and how to phrase them as actionable rules.

## OPENs — Unresolved. Flag if you encounter these during build.

- This is a Claud chat session in which claud chat will be given an industry an persona and a basic understanding of your idea. You will select an amount of time to pitch your idea and take questions. You can upload presentations or just chat. You will then be evaluated on your overall knowledge and ability to convince the persona that you have a solid idea _(from: Pitch challenge - You have a set amount of team (5,10,15,20) minutes to pitch your idea to a specific persona. A rubric will be used to evaluate how well you understand your idea, the market you are addressing, the economics behind it. It is a bit like shark tank but you get to determine who your audience is)_
- How does CC handle the complexity line between full lifecycle and fast path? Where exactly does "changes product direction" start? Is there a heuristic CC can apply, or is it always a developer judgment call? _(from: Session tab)_
- Should Chat ever produce code changes, or is that a hard boundary? Middle ground: Chat produces code suggestions that only become real when committed through Code? _(from: Session tab)_
- What happens to completion files in the repo over time? Archive strategy? Pruning? _(from: Ingestion Pipeline)_
- How does CC handle completion files from multiple repos simultaneously? _(from: Ingestion Pipeline)_
- GitHub API rate limiting: polling multiple repos on every dashboard visit could hit rate limits. Should we add a cooldown or last-polled timestamp beyond the current 60-second cooldown? _(from: Ingestion Pipeline)_
- Should the detection dialog suppress if user has already dismissed all new jobs in the current session? _(from: Ingestion Pipeline)_
- ODRC parser error tolerance: Chat output may have diverse formats. Should we add a manual entry fallback for unparseable lines? _(from: Ingestion Pipeline)_
- Batch classification size: no limit on number of jobs in a batch. For large batches, the Claude API call may exceed token limits. Should we add batching within batches? _(from: Ingestion Pipeline)_
- What does Chat's validation process look like in practice? Which checks run every time vs. triaged by complexity? _(from: Ingestion Pipeline)_
- When Chat challenges Code's work, what's the resolution path? CC creates a rework task for Code? CC creates a new OPEN? CC flags for developer decision? _(from: Ingestion Pipeline)_
- Does Chat need actual diffs attached to the validation bundle, or is the completion file + code sufficient? _(from: Ingestion Pipeline)_
- What is the exact structured output format Chat should use for ODRC recommendations? Needs a formal schema for reliable CC parsing. _(from: Ingestion Pipeline)_
- How does the developer get Chat's structured output back into CC? Phase 3 implements copy-paste into the Import modal. File upload? Direct API integration? _(from: Ingestion Pipeline)_
- Bundle size interactive selector — current implementation auto-excludes files over limit with manifest notes. Should there be an interactive file selector before assembly starts? _(from: Ingestion Pipeline)_
- Review prompt quality: using Haiku for cost efficiency. Should there be a model selector in settings for users who want higher quality prompts? _(from: Ingestion Pipeline)_
- Orphan commit rate limiting: polling 30 commits + detail fetch per commit could hit GitHub API rate limits. Should we add commit-level caching? _(from: Ingestion Pipeline)_
- Reconstruction task delivery: currently clipboard-only. Should we add repo push to cc/tasks/ so Code can pick it up automatically? _(from: Ingestion Pipeline)_
- When should the multi-phase spec packaging and job queue feature be built? _(from: Session tab)_
- Manifest format — JSON or markdown? Current lean: JSON (easier to parse). _(from: Session tab)_
- How does CC ingest the spec package? Developer uploads zip to CC? CC detects it in a repo directory? Chat pushes via some mechanism? _(from: Session tab)_
- How should CC handle test result files in cc/tests/results/ that don't match any completion file? _(from: Test Infrastructure)_
- Should test failure change the urgency of Chat validation? Auto-suggest Package for Check when a completion file reports failed tests? _(from: Test Infrastructure)_
- Test trend analytics — pass rate over time, test count growth, flaky test detection. When does this become valuable enough to build? _(from: Test Infrastructure)_
- Should CC validate that tests were actually run? Completion file claims passed but no results file exists. _(from: Test Infrastructure)_
- Should task_type and tracking be formally added to the completion file spec? _(from: Ingestion Pipeline)_
- How does CC's Job History display and filter by task type? _(from: Ingestion Pipeline)_
- External system integration (Jira, GitHub Issues) — what does export/sync look like? _(from: Ingestion Pipeline)_
- How does CC generate and dispatch bug fix specs? Quick-action from Dashboard? Button in Job History? _(from: Ingestion Pipeline)_
- Prop drilling is at its practical limit — DashboardView now has 30+ props. At what point does CC need React Context or another pattern? _(from: Ideation Platform)_
- appIdeas index consistency — could get out of sync if writes fail partway. Does it need a sync check? _(from: Ideation Platform)_
- Concept conflict detection in aggregate view — how to handle a RULE from Idea 1 contradicting a DECISION from Idea 3. _(from: Ideation Platform)_
- The codebase is now ~24K+ lines and growing. Should satellite extraction be prioritized? _(from: CC Architecture Rules)_
- ClaudeAPIService uses anthropic-dangerous-direct-browser-access header for browser-based API calls. Is this the long-term pattern or should calls go through Firebase Functions? _(from: CC Architecture Rules)_
- The js-yaml CDN adds a new external dependency. Should we vendor it or keep CDN? _(from: CC Architecture Rules)_
- CC should auto-populate base RULEs and CONSTRAINTs when an app is created, based on stack tags. Three tiers: Universal, Stack-specific, App-specific. When should this be built? _(from: CC Architecture Rules)_
- When CC blocks a Chat artifact placement, what's the recovery path? CC shows the diff and developer reconciles? CC regenerates the request with current state? CC routes to Code for integration? _(from: Session tab)_
- What metadata does CC track for placement validation? Commit SHA at artifact generation time? File-level hashes? Something simpler? _(from: Session tab)_
- How does industry context get incorporated into the persona prompt — free text field or structured selection? Industry shapes what credible answers sound like _(from: Defined Skills for Cloud Chat)_
- What does the system prompt structure look like to keep Claude in challenge mode without drifting to helpful coaching? Live test showed this is a real risk _(from: Defined Skills for Cloud Chat)_
- How does CC surface stress test suggestions at the right lifecycle moments without mandating them? Needs UX design for the nudge _(from: Defined Skills for Cloud Chat)_
- How does persistent OPEN tracking work in Phase 2 — distinguishing new OPENs vs unresolved OPENs carried forward vs OPENs promoted to DECISIONs since last stress test? _(from: Defined Skills for Cloud Chat)_
- Can persona questions be designed to intentionally surface all four ODRC categories, not just OPENs? Live test showed the persona mostly probed for weaknesses and missed opportunities to draw out constraints and rules _(from: Defined Skills for Cloud Chat)_
- Brief output format needs enrichment — current expected output template is too minimal to produce the quality of session documents needed for CC ingestion _(from: Defined Skills for Cloud Chat)_
- How deep does the historical context in the brief go? Options: full handoff from every prior session (gets heavy fast), rolling summary that CC compresses over time (requires summarization logic), or tiered approach — full recent handoff, compressed summaries for 2-3 sessions before that, one-paragraph origin story for older history. Needs to balance context richness against brief size budget. _(from: Defined Skills for Cloud Chat)_
- What specific skills comprise the base session package? Categories identified: ODRC model and extraction rules, session manifest protocol, session open procedures, session close procedures. Need to write the actual skill definitions — this is the next implementation step. _(from: Defined Skills for Cloud Chat)_
- What session types exist beyond stress test and exploration, and what is each type's specific skill mapping? Previously named: technical deep-dive, competitive analysis, voice of customer, economics. Full menu and per-type skill requirements are undefined. _(from: Defined Skills for Cloud Chat)_
- How does CC manage the skill registry — creation, versioning, and mapping of skills to session types? This is the CC engineering work to integrate with the Skills API. Includes initial skill creation, version management when protocols evolve, and the session-type-to-skill mapping logic. _(from: Defined Skills for Cloud Chat)_
- Does the Skills API skill attachment work with Claude.ai manual chat sessions or only with direct API calls? CC makes API calls for brief generation, but when the developer starts a manual chat session in Claude.ai, the skill attachment mechanism may differ. Needs technical validation. _(from: Defined Skills for Cloud Chat)_
- Session close skill template — what is the exact procedure Chat follows at session close? Needs to define: re-read the skill, read the session manifest, verify all artifacts are accounted for, produce ODRC updates in standard format, produce session handoff in standard format, assemble and present the complete package. This skill needs to be written and tested. _(from: Defined Skills for Cloud Chat)_
- Exact card layout and fields per card type — what minimal information makes each card type (Idea, App, Project) recognizable and launchable at a glance? Needs design session." → Resolved for Idea cards. MVP card fields: Idea name (linked to Idea page), app tag, phase badge with color-coded left stripe, last session number (linked to session detail), full date/time of last session, unresolved OPEN count. Single "Continue" button launches the session brief modal. No overflow menu — inline links handle navigation to deeper views. App and Project card designs deferred to future session. _(from: Session tab)_
- How will external inputs (Jira issues, bug reports, production alerts) eventually create or reprioritize cards on the landing page? This is a future capability but the card model should accommodate it. _(from: Session tab)_
- App launch capability on Projects page — version labels exist today but need to be made clickable/launchable. What's the implementation scope? _(from: Session tab)_
- What does a minimal session brief look like now that behavioral load has moved to skills? Need to design the brief template that just says 'read these skills, here's the context, here's the goal.' _(from: Defined Skills for Cloud Chat)_
- How does CC author and version skill zip files in its UI? Need a skill management view where skills can be created, edited, versioned, and exported as zip files for upload. _(from: Defined Skills for Cloud Chat)_
- How do we test that the skills actually influence Claude's behavior in Chat? Need a test protocol — upload skills, start a session with a brief that references them, evaluate whether Claude follows the skill instructions. _(from: Defined Skills for Cloud Chat)_
- Should session-continuity skill be absorbed into cc-session-structure or remain separate? Currently compaction recovery is included in session-structure. _(from: Defined Skills for Cloud Chat)_
- What are the exact lens definitions for Economics, Competitive, Voice of Customer, and Operational? Technical lens is built — remaining four need the same level of probing framework and question patterns. _(from: Defined Skills for Cloud Chat)_
- What are the exact mode definitions for Stress Test, Spec, and Review? Exploration mode is built — remaining three need the same level of posture and flow definition. _(from: Defined Skills for Cloud Chat)_
- What does idea management look like in CC — merging ideas, parent/child hierarchies, cross-idea ODRC reconciliation? (parked for future) _(from: Session tab)_
- Should CC preserve idea evolution history (renames, reframes, pivots) as a first-class feature? The journey from initial concept to final form has value. (parked for future) _(from: Session tab)_
- What is the exact spec format template and closing checklist content to embed in (a) the IdeationBriefGenerator spec-type system prompt and (b) the cc-mode-spec skill? This needs to be defined in Unit 3 (skill updates) and validated against the CLAUDE.md locked template to ensure field-level alignment." [Affinity: session-tab] _(from: Session tab)_
- What is the exact wording for the Required Attachments section and handshake prompt update in the brief generator for spec and claude-md sessions? Should it name specific files (e.g., attach index.html) or be generic (attach the current codebase)? Implementation belongs in the IdeationBriefGenerator updates." [Affinity: session-tab] _(from: Session tab)_
- What is the full taxonomy of spec review findings? Initial candidates: stale_reference (line numbers moved), signature_mismatch (function/prop API doesn't match spec), dead_code (UI for unbuilt backend), convention_drift (spec uses different patterns than repo), missing_context (spec assumes something not in codebase), scope_overreach (spec asks for more than one session can build). What severity model maps to these — warning, review, intervention? _(from: Spec Quality Feedback Loop)_
- What instructions should the CLAUDE.md template give Code about the pre-read review? Code already does a 'Before You Begin' assessment naturally. Should the template explicitly instruct Code to write the review file before building, or should it be a standing instruction in Code's own configuration? _(from: Spec Quality Feedback Loop)_
- Where does the review file live and what triggers action? Proposed: cc/specpreread/{spec-id}-review.md. But who reads it — does CC surface it on the next session card? Does the brief generator pull findings into the next spec session? Does the developer get a notification? _(from: Spec Quality Feedback Loop)_
- Should the severity classification drive automated workflow changes? E.g., 'intervention' findings could block the build and route back to Chat for revision, 'review' findings get surfaced to the developer for a go/no-go decision, 'warning' findings are informational and Code proceeds. _(from: Spec Quality Feedback Loop)_
- How does this interact with the completion file? The completion file already has unexpected_findings and new_opens fields. Should spec_quality be a new section in the completion file (post-build), or is the specpreread file (pre-build) sufficient, or do we need both — pre-build review + post-build actuals? _(from: Spec Quality Feedback Loop)_
- What does the feedback loop look like end-to-end? Chat produces spec → Code reviews → findings classified → developer decides → Code builds (or spec returns to Chat). How many round-trips is acceptable? Is one review pass sufficient or does this become iterative? _(from: Spec Quality Feedback Loop)_
- Should the Context Package and Session Brief be merged into a single ZIP download, and if so, what's the combined structure (SESSION_BRIEF.md + CONTEXT_PACKAGE.md + ODRC snapshots + app config)? _(from: Session tab)_
- What return-path instructions should be appended to session briefs so Claude Chat knows how to package output for CC ingestion (expected files, formats, naming conventions)? _(from: Session tab)_
- Can the two-step prompt protocol (handshake prompt then session brief) be simplified into a single paste, or does the UX just need better guidance (e.g., a separator with instructions)? _(from: Session tab)_
- How should hybrid ZIPs (containing both session.json and deployable code) be detected and dual-processed — session import plus deploy queue? _(from: Session tab)_
- What's the structured return format for maker/checker exercises? Should a CHECKER_RESULT.json be defined with pass/fail, issues found, and recommended fixes? _(from: Session tab)_
- Can orphan completion files be auto-matched to recent jobs by repo name, file paths, or timestamp proximity before requiring manual resolution? _(from: Session tab)_
- What actions should be available for session artifact files — save to idea as attachment, push to repo, copy to clipboard? Where in the UI do those actions surface? _(from: Session tab)_
- ODRC parser silently drops lines with non-standard format (e.g., 'ADD OPEN:' instead of 'ADD OPEN -'). Should the parser add fuzzy matching with a 'couldn't parse these — did you mean?' confirmation section in the import modal? _(from: Ingestion Pipeline)_
- When a dropped file yields zero parsed ODRC items, no feedback is shown — the drop silently does nothing. What feedback surface should appear (toast, modal, inline message) to tell the user nothing was detected? _(from: Ingestion Pipeline)_
- Pending ODRC imports are stored in React state only and lost on page refresh. Should pending imports be persisted to sessionStorage so a refresh doesn't lose them? _(from: Ingestion Pipeline)_
- Only one pending ODRC import is supported at a time — dropping a second file overwrites the first. Should imports be queued so multiple drops can be reviewed and processed sequentially? _(from: Ingestion Pipeline)_
- Post-Task Obligations template references '.cc/completions/' (with leading dot) but CC polling code checks 'cc/completions/' (no dot). Which path should be standardized, and where do all references need to be updated? _(from: Ingestion Pipeline)_
- Completion file detection only fires when the user navigates to dashboard or jobHistory views. Should background polling run on a configurable interval (e.g., 60 seconds) regardless of active view, with toast notifications for new completions? _(from: Ingestion Pipeline)_
- If Claude Code writes a malformed completion file (bad YAML, wrong fields), CC's parser may silently fail or show partial data. Should invalid completion files appear in a 'needs attention' section with the parse error? _(from: Ingestion Pipeline)_
- The includeCodebase checkbox in ExploreInChatModal renders but has no code path — toggling it does nothing. Should it be wired up to include relevant files in the session ZIP, or removed to avoid confusion? _(from: Ingestion Pipeline)_
- Should there be a periodic cleanup Cloud Function that scans for expired documents (past their TTL) and removes them? Or is TTL enforcement only at read-time (filter out expired docs from list results, lazy-delete when encountered)? Cloud Function adds infrastructure complexity but ensures Firebase stays clean even if nobody reads. Lazy-delete is simpler but stale data accumulates until queried. _(from: Document Queue Lifecycle — TTL, Auto-Cleanup, and Message Hygiene)_
- Should the infrastructure skill be dynamically generated from MCP server state (tool list, skill list, app list) or manually authored? Dynamic means it's always current but adds generation complexity. Manual means it's a static skill file that needs updating when the server changes. Lean: manual for v1, since infrastructure changes are infrequent and the skill content needs editorial judgment about what matters. _(from: Code Cold Start Protocol)_
- The CLAUDE.md generator is producing duplicated E2E test concepts (same RULE appears 6+ times) and 101 OPENs from across all ideas linked to the app. This noise degrades Code's ability to find signal. Should the generator be updated to deduplicate, filter by relevance to the active idea, or cap OPEN count? Separate from the cold start problem but directly related — a cleaner CLAUDE.md means less recovery burden. _(from: Code Cold Start Protocol)_
- Define the session start flow: User enters a CC project, which implies some form of ideation work (otherwise they'd use plain Chat). A session is always created immediately for context tracking. The user is then asked: formal ideation or general chat? This sets the mode. General chat = lightweight tracking (context estimate, timestamps, summary). Formal ideation = full ODRC protocol with check-ins, lenses, structured close. Mode can escalate mid-conversation if a general chat evolves into deeper exploration. Open questions: What's the exact UX for the mode prompt? Can mode de-escalate? How does this interact with the existing session protocol steps? _(from: Context Window Longevity — Lean Startup, Skill Tiering, and Hygiene Checks)_
- Investigate server-side periodic cleanup for Firebase. Auto-complete sessions older than N days with 0 concepts, purge delivered/failed documents past retention, clean up orphaned test data. Could be a Cloud Function on a schedule or a maintenance command in the MCP server. Scope and triggers TBD. _(from: CC Maintenance Backlog)_
- Job completion must trigger idea status review. Currently jobs complete but linked ideas remain active indefinitely with no lifecycle signal. The system needs: (1) on job completion, check if idea has remaining active OPENs or queued jobs — if not, flag for graduation/archive, (2) backfill existing completed jobs against their linked ideas to reconcile stale active statuses, (3) define what "idea done" means — all OPENs resolved + no active/draft jobs + Chat confirmation. _(from: CC Maintenance Backlog)_
- Firebase RTDB shared paths under command-center/ (config, deploy-history, session-log, deletion-history, rollback-snapshots, rules-history) are readable AND writable by any authenticated user. Per-user data under command-center/{uid}/ is properly isolated. Shared paths need to either move under {uid}/ scoping or get owner-restricted write rules before multi-user rollout. Not blocking for first tester but must be resolved before wider access. _(from: Cc Navigation Redesign)_
